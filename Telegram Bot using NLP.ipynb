{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TELEGRAM BOT USING NLP=DAY 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   #Package for NLP\n",
    "import re   #Library for regex\n",
    "import heapq  \n",
    "import numpy as np\n",
    "\n",
    "paragraph = \"\"\"Thank you all so very much.thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this:It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W',\" \",dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+',\" \",dataset[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word]=1\n",
    "        else:\n",
    "            word2count[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = heapq.nlargest(100,word2count,key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    x.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"\"\"I am Crickbot. How can i help you?. My mother's name is Aditi and i don't have a father. \n",
    "I'm a big time MS Dhoni fan. I love sharing facts about Indian Cricket Team. Captain of Indian Cricket Team is Virat Kohli.\n",
    "Coach of the current Indian Cricket Team is Ravi Shatri.\n",
    "\n",
    "Indian Cricket team won ICC Cricket World Cup in 2011.\n",
    "MS Dhoni was the captain of the team when team won the world cup 2011. \n",
    "Indian Cricket team won ICC Cricket World T20 Cup in 2007.\n",
    "MS Dhoni was the captain of the team when team won the world cup 2007. \n",
    "ICC Ranking for India Cricket Team  for Test is currently 3rd.\n",
    "ICC Ranking for India Cricket Team  for ODI is currently 2nd.\n",
    "ICC Ranking for India Cricket Team  for T20 is currently 3rd.\n",
    "Last T20I match by Indian Cricket team was played against New Zealand at Bay Oval, Mount Maunganui; 2 February 2020\n",
    "\n",
    "Last ODI match by Indian Cricket team was played against New Zealand at Bay Oval, Mount Maunganui; 11 February 2020\n",
    "\n",
    "Last Test match by Indian Cricket team was played against New Zealand at Hagley Oval, Christchurch; 29 February -2 March 2020.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LemTokens(tokens):\n",
    "    return [lemma.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response = \"\"\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = Normalize, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1],tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if (req_tfidf == 0):\n",
    "        robo_response = robo_response + \"Hey how're you doing!\"\n",
    "    else:\n",
    "        robo_response = robo_response + sent_tokens[idx]\n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_rep(user_q):\n",
    "    flag=True\n",
    "    #print(\"MAD: My name is MADbot. I will answer everything about myself. If you want to exit, type Bye!\")\n",
    "    while(flag==True):\n",
    "        user_response = user_q\n",
    "        user_response=user_response.lower()\n",
    "        if(user_response!='bye'):\n",
    "            if(user_response=='thanks' or user_response=='thank you' ):\n",
    "                flag=False\n",
    "                #print(\"MAD: You are welcome..\")\n",
    "                return (\"You are Welcome\")\n",
    "            else:\n",
    "                #print(\"MAD: \",end=\"\")\n",
    "                #print(response(user_response))\n",
    "                repo = (response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "                return repo\n",
    "\n",
    "                        \n",
    "        else:\n",
    "            flag=False\n",
    "            #print(\"MAD: Bye! Stay MAD Stay Creative.\")\n",
    "            return (\"Bye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "class telegram_bot():\n",
    "    def __init__(self):\n",
    "        self.token = \"1266343567:AAH58xlY-jBVJBiDAwaRhubxfOKW0M5N6fk\"    #write your token here!  \n",
    "    \n",
    "        self.url = f\"https://api.telegram.org/bot{self.token}\"\n",
    "\n",
    "    def get_updates(self,offset=None):\n",
    "        url = self.url+\"/getUpdates?timeout=100\"\n",
    "        if offset:\n",
    "            url = url+f\"&offset={offset+1}\"\n",
    "        url_info = requests.get(url)\n",
    "        return json.loads(url_info.content)\n",
    "\n",
    "    def send_message(self,msg,chat_id):\n",
    "        url = self.url + f\"/sendMessage?chat_id={chat_id}&text={msg}\"\n",
    "        if msg is not None:\n",
    "            print(requests.get(url))\n",
    "\n",
    "    def grab_token(self):\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbot = telegram_bot()\n",
    "\n",
    "update_id = None\n",
    "\n",
    "def make_reply(msg):\n",
    "    if msg is not None:\n",
    "        reply = bot_rep(message)\n",
    "    return reply\n",
    "\n",
    "while True:\n",
    "    print(\"...\")\n",
    "    updates = tbot.get_updates(offset=update_id)\n",
    "    updates = updates['result']\n",
    "    print(updates)\n",
    "    if updates:\n",
    "        for item in updates:\n",
    "            update_id = item[\"update_id\"]\n",
    "            print(update_id)\n",
    "            try:\n",
    "                message = item[\"message\"][\"text\"]\n",
    "                print(message)\n",
    "            except:\n",
    "                message = None\n",
    "            from_ = item[\"message\"][\"from\"][\"id\"]\n",
    "            print(from_)\n",
    "\n",
    "            reply = make_reply(message)\n",
    "            tbot.send_message(reply,from_)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
