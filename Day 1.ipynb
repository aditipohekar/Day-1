{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1 style=\"color:red\">The AI CHATBOT: DAY 1</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tokenizing words and sentences</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk   #Package for NLP\n",
    "import numpy as np\n",
    "nltk.download('punkt')   #It contains other dictionaries for stemming and other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "paragraph = \"\"\"Thank you all so very much. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow.  We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)  # Sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you all so very much.',\n",
       " 'Thank you to everybody at \\n               Fox and New Regency … my entire team.',\n",
       " 'I have to thank \\n               everyone from the very onset of my career … To my parents; \\n               none of this would be possible without you.',\n",
       " 'And to my \\n               friends, I love you dearly; you know who you are.',\n",
       " \"And lastly,\\n               I just want to say this: Making The Revenant was about\\n               man's relationship to the natural world.\",\n",
       " 'A world that we\\n               collectively felt in 2015 as the hottest year in recorded\\n               history.',\n",
       " 'Our production needed to move to the southern\\n               tip of this planet just to be able to find snow.',\n",
       " 'We need to\\n               support leaders around the world who do not speak for the \\n               big polluters, but who speak for all of humanity, for the\\n               indigenous people of the world, for the billions and \\n               billions of underprivileged people out there who would be\\n               most affected by this.',\n",
       " 'I thank you all for this \\n               amazing award tonight.',\n",
       " 'Let us not take this planet for \\n               granted.',\n",
       " 'I do not take tonight for granted.',\n",
       " 'Thank you so very much.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences #See sentences are tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)   #Word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank',\n",
       " 'you',\n",
       " 'all',\n",
       " 'so',\n",
       " 'very',\n",
       " 'much',\n",
       " '.',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'to',\n",
       " 'everybody',\n",
       " 'at',\n",
       " 'Fox',\n",
       " 'and',\n",
       " 'New',\n",
       " 'Regency',\n",
       " '…',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'team',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'to',\n",
       " 'thank',\n",
       " 'everyone',\n",
       " 'from',\n",
       " 'the',\n",
       " 'very',\n",
       " 'onset',\n",
       " 'of',\n",
       " 'my',\n",
       " 'career',\n",
       " '…',\n",
       " 'To',\n",
       " 'my',\n",
       " 'parents',\n",
       " ';',\n",
       " 'none',\n",
       " 'of',\n",
       " 'this',\n",
       " 'would',\n",
       " 'be',\n",
       " 'possible',\n",
       " 'without',\n",
       " 'you',\n",
       " '.',\n",
       " 'And',\n",
       " 'to',\n",
       " 'my',\n",
       " 'friends',\n",
       " ',',\n",
       " 'I',\n",
       " 'love',\n",
       " 'you',\n",
       " 'dearly',\n",
       " ';',\n",
       " 'you',\n",
       " 'know',\n",
       " 'who',\n",
       " 'you',\n",
       " 'are',\n",
       " '.',\n",
       " 'And',\n",
       " 'lastly',\n",
       " ',',\n",
       " 'I',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'say',\n",
       " 'this',\n",
       " ':',\n",
       " 'Making',\n",
       " 'The',\n",
       " 'Revenant',\n",
       " 'was',\n",
       " 'about',\n",
       " 'man',\n",
       " \"'s\",\n",
       " 'relationship',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'world',\n",
       " '.',\n",
       " 'A',\n",
       " 'world',\n",
       " 'that',\n",
       " 'we',\n",
       " 'collectively',\n",
       " 'felt',\n",
       " 'in',\n",
       " '2015',\n",
       " 'as',\n",
       " 'the',\n",
       " 'hottest',\n",
       " 'year',\n",
       " 'in',\n",
       " 'recorded',\n",
       " 'history',\n",
       " '.',\n",
       " 'Our',\n",
       " 'production',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'move',\n",
       " 'to',\n",
       " 'the',\n",
       " 'southern',\n",
       " 'tip',\n",
       " 'of',\n",
       " 'this',\n",
       " 'planet',\n",
       " 'just',\n",
       " 'to',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'find',\n",
       " 'snow',\n",
       " '.',\n",
       " 'We',\n",
       " 'need',\n",
       " 'to',\n",
       " 'support',\n",
       " 'leaders',\n",
       " 'around',\n",
       " 'the',\n",
       " 'world',\n",
       " 'who',\n",
       " 'do',\n",
       " 'not',\n",
       " 'speak',\n",
       " 'for',\n",
       " 'the',\n",
       " 'big',\n",
       " 'polluters',\n",
       " ',',\n",
       " 'but',\n",
       " 'who',\n",
       " 'speak',\n",
       " 'for',\n",
       " 'all',\n",
       " 'of',\n",
       " 'humanity',\n",
       " ',',\n",
       " 'for',\n",
       " 'the',\n",
       " 'indigenous',\n",
       " 'people',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'for',\n",
       " 'the',\n",
       " 'billions',\n",
       " 'and',\n",
       " 'billions',\n",
       " 'of',\n",
       " 'underprivileged',\n",
       " 'people',\n",
       " 'out',\n",
       " 'there',\n",
       " 'who',\n",
       " 'would',\n",
       " 'be',\n",
       " 'most',\n",
       " 'affected',\n",
       " 'by',\n",
       " 'this',\n",
       " '.',\n",
       " 'I',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'all',\n",
       " 'for',\n",
       " 'this',\n",
       " 'amazing',\n",
       " 'award',\n",
       " 'tonight',\n",
       " '.',\n",
       " 'Let',\n",
       " 'us',\n",
       " 'not',\n",
       " 'take',\n",
       " 'this',\n",
       " 'planet',\n",
       " 'for',\n",
       " 'granted',\n",
       " '.',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'take',\n",
       " 'tonight',\n",
       " 'for',\n",
       " 'granted',\n",
       " '.',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'so',\n",
       " 'very',\n",
       " 'much',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Stemming</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') #LIBRARY HAVING STEMMED WORDS (DICTIONARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)  #Step1 : Tokenizing it by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you all so very much.',\n",
       " 'Thank you to everybody at \\n               Fox and New Regency … my entire team.',\n",
       " 'I have to thank \\n               everyone from the very onset of my career … To my parents; \\n               none of this would be possible without you.',\n",
       " 'And to my \\n               friends, I love you dearly; you know who you are.',\n",
       " \"And lastly,\\n               I just want to say this: Making The Revenant was about\\n               man's relationship to the natural world.\",\n",
       " 'A world that we\\n               collectively felt in 2015 as the hottest year in recorded\\n               history.',\n",
       " 'Our production needed to move to the southern\\n               tip of this planet just to be able to find snow.',\n",
       " 'We need to\\n               support leaders around the world who do not speak for the \\n               big polluters, but who speak for all of humanity, for the\\n               indigenous people of the world, for the billions and \\n               billions of underprivileged people out there who would be\\n               most affected by this.',\n",
       " 'I thank you all for this \\n               amazing award tonight.',\n",
       " 'Let us not take this planet for \\n               granted.',\n",
       " 'I do not take tonight for granted.',\n",
       " 'Thank you so very much.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer  #importing PorterStemmer Class from nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()  #creating objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thank you all so veri much .', 'thank you to everybodi at fox and new regenc … my entir team .', 'I have to thank everyon from the veri onset of my career … To my parent ; none of thi would be possibl without you .', 'and to my friend , I love you dearli ; you know who you are .', \"and lastli , I just want to say thi : make the reven wa about man 's relationship to the natur world .\", 'A world that we collect felt in 2015 as the hottest year in record histori .', 'our product need to move to the southern tip of thi planet just to be abl to find snow .', 'We need to support leader around the world who do not speak for the big pollut , but who speak for all of human , for the indigen peopl of the world , for the billion and billion of underprivileg peopl out there who would be most affect by thi .', 'I thank you all for thi amaz award tonight .', 'let us not take thi planet for grant .', 'I do not take tonight for grant .', 'thank you so veri much .']\n"
     ]
    }
   ],
   "source": [
    "print(sentences) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lemmatization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')  #DICTIONARY FOR POS TAGGING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('MAD', 'JJ'), ('Platform', 'NN')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "lem = WordNetLemmatizer()\n",
    "sent = \"this is a MAD Platform\"\n",
    "pos_tag(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "be\n",
      "a\n",
      "MAD\n",
      "Platform\n"
     ]
    }
   ],
   "source": [
    "for word, tag in pos_tag(word_tokenize(sent)):\n",
    "    lemtag = tag[0].lower()\n",
    "    lemtag = lemtag if lemtag in ['a','r','n','v'] else None\n",
    "    if not lemtag:\n",
    "        lemma = word\n",
    "    else:\n",
    "        lemma = lem.lemmatize(word,lemtag)\n",
    "    print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>StopWords</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') #STOP WORDS DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank veri much .',\n",
       " 'thank everybodi fox new regenc … entir team .',\n",
       " 'I thank everyon veri onset career … To parent ; none thi would possibl without .',\n",
       " 'friend , I love dearli ; know .',\n",
       " \"lastli , I want say thi : make reven wa man 's relationship natur world .\",\n",
       " 'A world collect felt 2015 hottest year record histori .',\n",
       " 'product need move southern tip thi planet abl find snow .',\n",
       " 'We need support leader around world speak big pollut , speak human , indigen peopl world , billion billion underprivileg peopl would affect thi .',\n",
       " 'I thank thi amaz award tonight .',\n",
       " 'let us take thi planet grant .',\n",
       " 'I take tonight grant .',\n",
       " 'thank veri much .']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>POS TAGGING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)  #STEP1: Word Tokenize\n",
    "tagged_words = nltk.pos_tag(words)   #POS TAGGING MAIN STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('so', 'RB'),\n",
       " ('very', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('everybody', 'VB'),\n",
       " ('at', 'IN'),\n",
       " ('Fox', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('New', 'NNP'),\n",
       " ('Regency', 'NNP'),\n",
       " ('…', 'NNP'),\n",
       " ('my', 'PRP$'),\n",
       " ('entire', 'JJ'),\n",
       " ('team', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('thank', 'VB'),\n",
       " ('everyone', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('onset', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('career', 'NN'),\n",
       " ('…', 'NN'),\n",
       " ('To', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('parents', 'NNS'),\n",
       " (';', ':'),\n",
       " ('none', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('possible', 'JJ'),\n",
       " ('without', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('to', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('friends', 'NNS'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('dearly', 'RB'),\n",
       " (';', ':'),\n",
       " ('you', 'PRP'),\n",
       " ('know', 'VBP'),\n",
       " ('who', 'WP'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('lastly', 'RB'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('say', 'VB'),\n",
       " ('this', 'DT'),\n",
       " (':', ':'),\n",
       " ('Making', 'VBG'),\n",
       " ('The', 'DT'),\n",
       " ('Revenant', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('about', 'IN'),\n",
       " ('man', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('relationship', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('natural', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " ('.', '.'),\n",
       " ('A', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('collectively', 'RB'),\n",
       " ('felt', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('2015', 'CD'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('hottest', 'JJS'),\n",
       " ('year', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('recorded', 'JJ'),\n",
       " ('history', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Our', 'PRP$'),\n",
       " ('production', 'NN'),\n",
       " ('needed', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('move', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('southern', 'JJ'),\n",
       " ('tip', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('planet', 'NN'),\n",
       " ('just', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('find', 'VB'),\n",
       " ('snow', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('support', 'VB'),\n",
       " ('leaders', 'NNS'),\n",
       " ('around', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('speak', 'VB'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('polluters', 'NNS'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('who', 'WP'),\n",
       " ('speak', 'VBP'),\n",
       " ('for', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('humanity', 'NN'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('indigenous', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('billions', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('billions', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('underprivileged', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('out', 'IN'),\n",
       " ('there', 'EX'),\n",
       " ('who', 'WP'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('most', 'RBS'),\n",
       " ('affected', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('thank', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('for', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('amazing', 'JJ'),\n",
       " ('award', 'NN'),\n",
       " ('tonight', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Let', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('not', 'RB'),\n",
       " ('take', 'VB'),\n",
       " ('this', 'DT'),\n",
       " ('planet', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('granted', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('take', 'VB'),\n",
       " ('tonight', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('granted', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('so', 'RB'),\n",
       " ('very', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming paragraph into format word_posTag\n",
    "word_tags = []\n",
    "for tw in tagged_words:\n",
    "    word_tags.append(tw[0]+\"_\"+tw[1])\n",
    "    \n",
    "tag_para = ' '.join(word_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank_NNP you_PRP all_DT so_RB very_RB much_JJ ._. Thank_VB you_PRP to_TO everybody_VB at_IN Fox_NNP and_CC New_NNP Regency_NNP …_NNP my_PRP$ entire_JJ team_NN ._. I_PRP have_VBP to_TO thank_VB everyone_NN from_IN the_DT very_RB onset_NN of_IN my_PRP$ career_NN …_NN To_TO my_PRP$ parents_NNS ;_: none_NN of_IN this_DT would_MD be_VB possible_JJ without_IN you_PRP ._. And_CC to_TO my_PRP$ friends_NNS ,_, I_PRP love_VBP you_PRP dearly_RB ;_: you_PRP know_VBP who_WP you_PRP are_VBP ._. And_CC lastly_RB ,_, I_PRP just_RB want_VBP to_TO say_VB this_DT :_: Making_VBG The_DT Revenant_NNP was_VBD about_IN man_NN 's_POS relationship_NN to_TO the_DT natural_JJ world_NN ._. A_DT world_NN that_IN we_PRP collectively_RB felt_VBD in_IN 2015_CD as_IN the_DT hottest_JJS year_NN in_IN recorded_JJ history_NN ._. Our_PRP$ production_NN needed_VBN to_TO move_VB to_TO the_DT southern_JJ tip_NN of_IN this_DT planet_NN just_RB to_TO be_VB able_JJ to_TO find_VB snow_JJ ._. We_PRP need_VBP to_TO support_VB leaders_NNS around_IN the_DT world_NN who_WP do_VBP not_RB speak_VB for_IN the_DT big_JJ polluters_NNS ,_, but_CC who_WP speak_VBP for_IN all_DT of_IN humanity_NN ,_, for_IN the_DT indigenous_JJ people_NNS of_IN the_DT world_NN ,_, for_IN the_DT billions_NNS and_CC billions_NNS of_IN underprivileged_JJ people_NNS out_IN there_EX who_WP would_MD be_VB most_RBS affected_VBN by_IN this_DT ._. I_PRP thank_VBP you_PRP all_DT for_IN this_DT amazing_JJ award_NN tonight_NN ._. Let_VB us_PRP not_RB take_VB this_DT planet_NN for_IN granted_VBN ._. I_PRP do_VBP not_RB take_VB tonight_NN for_IN granted_VBN ._. Thank_NNP you_PRP so_RB very_RB much_JJ ._.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_para"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Name Entity Recognition</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Aditi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"The Taj Mahal was built by Emperor Shah Jahan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)\n",
    "tagged_words=  nltk.pos_tag(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedEnt = nltk.ne_chunk(tagged_words)\n",
    "namedEnt.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
